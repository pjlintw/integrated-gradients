# Integrated-Gradients for Conditional Text Generation

[**Data**](#dataset-and-preprocessing) | [**Conditional Text GAN**](#conditional-text-gans) | [**Integrated-Gradients**](#integrated-gradients)]


The repository works on training and analising the conditional text GANs where it is a Transformer-based generator  for the question generation. We leverage the explainable method, integrated gr adients, to analysis how discriminator distinguishes the real sentence from the artifical sentence generated by the generator.

We use `wikiTableQuestions` datasets to generate the question given certain question word. Our generator produce a question sentence to fool the discriminator. The dataset contains 130000 pairs of question sentence and anwser. We extract the question words from the questions. To reproduce the results, follow the steps below.

In the literature, the initial layers are used to encode general, semantic-irrelevant information. The middle layers usually enable them to produce information-rich representations. The latter layers are good at encoding the abstractive and task-oriented semantic representation. We develop a flexible framework to run such experiments. 

* New March 28th, 2021: Train conditional text GANs    


## Installation

### Python version

* Python >= 3.8

### Environment

Create an environment from file and activate the environment.

```
conda env create -f environment.yaml
conda activate fabian-pinjie
```

If conda fails to create an environment from `environment.yaml`. This may be caused by the platform-specific build constraints in the file. Try to create one by installing the important packages manually. The `environment.yaml` was built in macOS.

**Note**: Running `conda env export > environment.yaml` will include all the 
dependencies conda automatically installed for you. Some dependencies may not work in different platforms.
We suggest you to use the `--from-history` flag to export the packages to the environment setting file.
Make sure `conda` only exports the packages that you've explicitly asked for.

```
conda env export > environment.yaml --from-history
```

## Dataset and Preprocessing

### Dataset concatenation

We use the `wikiTableQuestions` dataset with the released version `1.0.2` as the exmples for training the transformer-based text GANs. The file are in the `data/wikiTable` folder. All examples store in `questions.tsv`. You can get the dataset `data/training.tsv` by the official repository [ppasupat/WikiTableQuestions](https://github.com/ppasupat/WikiTableQuestions). Rename the file as  `questions.tsv` and save it to data folder `data/wikiTable`.

To preprocess the dataset and fetch the statistical information regarding the features and labels. Move to the working directory `data`.

```
cd data
```

In the following steps, you will preprocess the collected file `sample.conll`, then split them into `sample.train`, `sample.dev` and `sample.test`
for building the datasets. You have to change the relative path to `--datasets_name` if you're using a different file directory .


### Preprocessing and Dataset Splitting

The file `questions.tsv` contains irrelevant information for training the neural nets.
We extract the questions and the corresponding question word as the labels. The generator in Text GANs is trained to generate the question given the labels.

Running `data_preprocess.py` to extract `label`, `sentence length` and `question`, then write them to `sample.tsv` in which the label and the features  are separated by tab. 

The arguments `--dataset_name` and `--output_dir` are the file to be passed to the program and the repository for the output file respectively. 

It generates `sample.tsv` for all examples and `sample.train`, `sample.dev` and `sample.test` for the network training.  The examples will be shuffled in the scripts and split into `train`, `validation` and `test` files.  The arguments `--eval_samples` and `--test_samples`
decide the number of samples will be selected from examples. In the datasets, we select 11779 for training set, 200 for validation and test sets respectively. To preprocess and split the datasets, you need to run the code below. 

```python
python data_preprocess.py \
  --dataset_name wikiTable/questions.tsv \
  --output_dir wikiTable \
  --eval_samples 200 \
  --test_samples 200
```

These output files for building datasets will under the path `--output_dir`. You will get the result.

```
Loading 12179 examples
Seed 49 is used to shuffle examples
Saving 12179 examples to wikiTable/sample.tsv
Saving 11779 examples to wikiTable/sample.train
Saving 200 examples to wikiTable/sample.dev
Saving 200 examples to wikiTable/sample.test
```

Make sure that you pass the correct **datasets** to the `--dataset_name`argument and it has enough examples for splitting out develop and test set. The output files may have no example, if the numbers of eval and test exmaples are more than the examples in the `questions.tsv`

### Data Information

To get the information regarding the questions and the question words. Execute the script `data_information.py` to compute 
the percentiles, maximum, minimum and mean of the question length, number of examples, label and its percentage.

The arguments `--dataset_name` and `output_dir` are the file to be passed to the program and the repository for the output file respectively. 

```python
python data_information.py \
  --dataset_name wikiTable/sample.tsv \
  --output_dir wikiTable/
```

The output file `sample.info` will be exported in the  `--output_dir` directory.



### Using dataset loading script for WikiTableQuestions

We use our dataset loading script `wiki-table-questions.py`for creating dataset. The script builds the train, validation and test sets from the 
dataset splits obtained by the `data_preprocess.py` program. 
Make sure the dataset split files `sample.train`, `sample.dev` , and `sample.test` are included in the datasets folder `data/wikiTable` your dataset folder.

If you get an error message like:

```
pyarrow.lib.ArrowTypeError: Could not convert 1 with type int: was not a sequence or recognized null for conversion to list type
```

You may have run other datasets in the same folder before. The Huggingface already created `.arrow` files once you run a loading script. These files are for reloading the datasets quickly.

Try to move the dataset you would like to use to the other folder and modify the path in the loading scipt. 

Or delete the relavent folder and files in the `.cache` for datasets. `cd ~/USERS_NAME/.cache/huggingface/datasets/` and `rm -r *`. This means that all the loading records will be removed and
 Hugginface will create the `.arrows` files again, including the previous laoding records. 


## Conditional Text GANs

We evaluate the BERT on linear probing test to see which layer capture more linguistic structure 
information in their contextual representations. The output layers for classifying the POS tags are added on the different layers of BERT. We only train these layer's weights.

We treat BERT as a feature extractor to provide fixed pre-trained contextual embeddings. We set `requires_grad` false for BERT model. If you would like to fine-tune the whole model, just comment those lines.

In certain cases, rather than fine-tuning the entire pre-trained model end-to-end, it can be beneficial to obtain pre-trained contextual embeddings, which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.

We found that executing a  64 minibatch size trained with maximal sequence length is pretty slow. 
The maximal sequence length, in OntoNotes is 228, is usually an extreme case. We gain huge improvement on the runtime for a minibatch by using 63 to `max_seq_length` covering 99% of sequence length.

ALL the examples below use the dataset loading script for `OntoNotes`. If yo would like to run your dataset for sequence labeling, consult the official [tutorial](https://huggingface.co/docs/datasets/add_dataset.html).

Note that dataset script and voacb file are required.

### Train Text GANs.

Conditional text GANs generate a textual sequence conditioned on given attribtuin.

-Linear-BERT is the feature-based approach with BERT
and an architecture to extract the fixed contextual representations from the BERT. It aims to evaluate which layer captures linguistic structure information among the different layers. We freeze BERTâ€™s weights and only train the classifier.

As a alternative, we uses an transformer for our generator.

-The custom model uses `bert-base-cased` as the base model follow by the dropout and linear classifier for labeling. You can specify the layer with the index from **0** to **12** to the argument `to_layer`. `0` indicates the embedding layer and the 12 BERT's layers are in the range of `1` to `12`. If you use a classifier on top of 12th BERT's layer, where you use `12` as the arugment. It is same as the standard BERT that `BertForTokenClassification` class creats for you. 

To train the architecture, you have to pass the arguments for dataset loading and vocab file. Use `vocab` and `dataset_script` arguments.
 
```python
 python run_trainer.py \
 --model_name_or_path textgan \
 --output_dir results/cgan-exp-1 \
 --vocab data/wikiTable/word.vocab \
 --dataset_script wiki-table-questions.py \
 --max_seq_length 20 \
 --batch_size 48 \
 --do_train True \
 --do_eval True \
 --do_predict True \
 --max_val_samples 200 \
 --max_test_samples 200 \
 --logging_first_step True \
 --logging_steps 50 \
 --eval_steps 10
```

### Integrated-Gradients on Text GANs

We apply integrated gradient technique to text GANs.

### Contact Information

For help or issues using the code, please submit a GitHub issue.
