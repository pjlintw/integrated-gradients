14:36:19, 664 __main__ INFO Namespace(batch_size=48, dataset_script='wiki-table-questions.py', do_eval=True, do_predict=True, do_train=True, eval_steps=10, gpu=False, logging_first_step='True', logging_steps=5, max_seq_length=20, max_steps=50, max_test_samples=200, max_train_samples=None, max_val_samples=200, mle_epochs=3, model_name_or_path='textgan', output_dir='results/cgan-exp-1', rnn_bidirectional=False, rnn_classes=1, rnn_dims=256, rnn_dropout_rate=0.1, rnn_embedding_dims=256, rnn_layers=1, tf_dims=512, tf_dropout_rate=0.1, tf_embedding_dims=256, tf_heads=8, tf_layers=2, tf_learning_rate=0.01, tf_shared_emb_layer=False, train_discriminator_epochs=1, vocab='data/wikiTable/word.vocab')
14:36:19, 665 __main__ INFO Saving hyperparameters to: results/cgan-exp-1/hyparams.txt
14:36:19, 676 datasets.builder WARNING Reusing dataset wiki_table_questions (/home/CE/pinjie/.cache/huggingface/datasets/wiki_table_questions/WikiTableQuestions/0.0.0/6a4116117d81a3d888218d55619bdb3ba81a1942ee880bf11b7f64a30e2f8e1a)
14:36:19, 689 __main__ INFO Loading Datasets
14:36:19, 698 root INFO PAD_IDX: 2439
14:36:22, 483 root INFO TransformerEncoder(
  (enc_layers): ModuleList(
    (0): TransformerEncLayer(
      (mha): MultiHeadAttention(
        (w_q): Linear(in_features=512, out_features=512, bias=True)
        (w_k): Linear(in_features=512, out_features=512, bias=True)
        (w_v): Linear(in_features=512, out_features=512, bias=True)
      )
      (ffn): FeedForwardBlock(
        (dense1): Linear(in_features=512, out_features=2048, bias=True)
        (relu): ReLU()
        (dense2): Linear(in_features=2048, out_features=512, bias=True)
      )
      (layernorm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (layernorm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
    (1): TransformerEncLayer(
      (mha): MultiHeadAttention(
        (w_q): Linear(in_features=512, out_features=512, bias=True)
        (w_k): Linear(in_features=512, out_features=512, bias=True)
        (w_v): Linear(in_features=512, out_features=512, bias=True)
      )
      (ffn): FeedForwardBlock(
        (dense1): Linear(in_features=512, out_features=2048, bias=True)
        (relu): ReLU()
        (dense2): Linear(in_features=2048, out_features=512, bias=True)
      )
      (layernorm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (layernorm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
14:36:22, 558 root INFO CustomLSTM(
  (embedding): Embedding(11471, 300)
  (lstm): LSTM(300, 128, batch_first=True, bidirectional=True)
  (drop): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=256, out_features=1, bias=True)
)
14:36:35, 379 root INFO Model: generator, Step: 50, Loss: 7.53
14:36:48, 151 root INFO Model: generator, Step: 100, Loss: 5.81
14:37:00, 762 root INFO Model: generator, Step: 150, Loss: 5.14
14:37:13, 512 root INFO Model: generator, Step: 200, Loss: 4.62
14:37:37, 645 root INFO Model: generator, Step: 50, Loss: 4.26
14:37:50, 227 root INFO Model: generator, Step: 100, Loss: 4.18
14:38:02, 762 root INFO Model: generator, Step: 150, Loss: 4.02
14:38:15, 286 root INFO Model: generator, Step: 200, Loss: 3.95
14:38:38, 946 root INFO Model: generator, Step: 50, Loss: 3.80
14:38:51, 595 root INFO Model: generator, Step: 100, Loss: 3.66
14:39:04, 9 root INFO Model: generator, Step: 150, Loss: 3.70
14:39:16, 476 root INFO Model: generator, Step: 200, Loss: 3.32
14:39:28, 235 root INFO Step: 1, Classifcation Loss: 33.11, Classification Acc: 27.00
14:39:28, 236 root INFO Step: 1, Matching Loss: 33.00, Matching Acc: 25.00
14:39:28, 826 root INFO Step: 2, Classifcation Loss: 33.58, Classification Acc: 20.00
14:39:28, 826 root INFO Step: 2, Matching Loss: 24.90, Matching Acc: 38.00
14:39:29, 439 root INFO Step: 3, Classifcation Loss: 33.37, Classification Acc: 22.00
14:39:29, 439 root INFO Step: 3, Matching Loss: 29.91, Matching Acc: 33.00
14:39:30, 20 root INFO Step: 4, Classifcation Loss: 33.29, Classification Acc: 22.00
14:39:30, 20 root INFO Step: 4, Matching Loss: 21.84, Matching Acc: 39.00
14:39:30, 615 root INFO Step: 5, Classifcation Loss: 33.75, Classification Acc: 20.00
14:39:30, 615 root INFO Step: 5, Matching Loss: 21.35, Matching Acc: 41.00
14:39:31, 232 root INFO Step: 6, Classifcation Loss: 33.24, Classification Acc: 23.00
14:39:31, 232 root INFO Step: 6, Matching Loss: 24.29, Matching Acc: 39.00
14:39:31, 802 root INFO Step: 7, Classifcation Loss: 33.57, Classification Acc: 21.00
14:39:31, 802 root INFO Step: 7, Matching Loss: 18.73, Matching Acc: 41.00
14:39:32, 435 root INFO Step: 8, Classifcation Loss: 32.96, Classification Acc: 25.00
14:39:32, 435 root INFO Step: 8, Matching Loss: 17.45, Matching Acc: 42.00
14:39:33, 23 root INFO Step: 9, Classifcation Loss: 33.23, Classification Acc: 28.00
14:39:33, 23 root INFO Step: 9, Matching Loss: 15.04, Matching Acc: 42.00
14:39:33, 687 root INFO Step: 10, Classifcation Loss: 33.50, Classification Acc: 24.00
14:39:33, 687 root INFO Step: 10, Matching Loss: 13.22, Matching Acc: 43.00
14:39:34, 295 root INFO Step: 11, Classifcation Loss: 33.53, Classification Acc: 20.00
14:39:34, 296 root INFO Step: 11, Matching Loss: 9.07, Matching Acc: 47.00
14:39:34, 907 root INFO Step: 12, Classifcation Loss: 33.80, Classification Acc: 18.00
14:39:34, 907 root INFO Step: 12, Matching Loss: 14.47, Matching Acc: 44.00
14:39:35, 466 root INFO Step: 13, Classifcation Loss: 34.08, Classification Acc: 18.00
14:39:35, 466 root INFO Step: 13, Matching Loss: 15.78, Matching Acc: 41.00
14:39:35, 983 root INFO Step: 14, Classifcation Loss: 33.80, Classification Acc: 18.00
14:39:35, 983 root INFO Step: 14, Matching Loss: 11.72, Matching Acc: 44.00
14:39:36, 563 root INFO Step: 15, Classifcation Loss: 33.00, Classification Acc: 24.00
14:39:36, 563 root INFO Step: 15, Matching Loss: 7.93, Matching Acc: 46.00
14:39:37, 216 root INFO Step: 16, Classifcation Loss: 33.77, Classification Acc: 19.00
14:39:37, 216 root INFO Step: 16, Matching Loss: 9.27, Matching Acc: 46.00
14:39:37, 783 root INFO Step: 17, Classifcation Loss: 34.03, Classification Acc: 20.00
14:39:37, 783 root INFO Step: 17, Matching Loss: 3.78, Matching Acc: 47.00
14:39:38, 392 root INFO Step: 18, Classifcation Loss: 32.87, Classification Acc: 28.00
14:39:38, 393 root INFO Step: 18, Matching Loss: 3.35, Matching Acc: 48.00
14:39:38, 923 root INFO Step: 19, Classifcation Loss: 33.52, Classification Acc: 20.00
14:39:38, 923 root INFO Step: 19, Matching Loss: 2.42, Matching Acc: 48.00
14:39:39, 479 root INFO Step: 20, Classifcation Loss: 33.26, Classification Acc: 24.00
14:39:39, 479 root INFO Step: 20, Matching Loss: 2.51, Matching Acc: 48.00
14:39:40, 25 root INFO Step: 21, Classifcation Loss: 33.21, Classification Acc: 23.00
14:39:40, 25 root INFO Step: 21, Matching Loss: 2.70, Matching Acc: 47.00
14:39:40, 671 root INFO Step: 22, Classifcation Loss: 33.51, Classification Acc: 19.00
14:39:40, 671 root INFO Step: 22, Matching Loss: 1.63, Matching Acc: 48.00
14:39:41, 231 root INFO Step: 23, Classifcation Loss: 32.71, Classification Acc: 28.00
14:39:41, 231 root INFO Step: 23, Matching Loss: 2.05, Matching Acc: 48.00
14:39:41, 805 root INFO Step: 24, Classifcation Loss: 33.20, Classification Acc: 25.00
14:39:41, 805 root INFO Step: 24, Matching Loss: 3.79, Matching Acc: 47.00
14:39:42, 391 root INFO Step: 25, Classifcation Loss: 33.42, Classification Acc: 19.00
14:39:42, 391 root INFO Step: 25, Matching Loss: 9.22, Matching Acc: 46.00
14:39:43, 4 root INFO Step: 26, Classifcation Loss: 32.89, Classification Acc: 23.00
14:39:43, 4 root INFO Step: 26, Matching Loss: 1.30, Matching Acc: 48.00
14:39:43, 552 root INFO Step: 27, Classifcation Loss: 33.92, Classification Acc: 22.00
14:39:43, 552 root INFO Step: 27, Matching Loss: 5.94, Matching Acc: 46.00
14:39:44, 140 root INFO Step: 28, Classifcation Loss: 33.89, Classification Acc: 19.00
14:39:44, 141 root INFO Step: 28, Matching Loss: 9.00, Matching Acc: 46.00
14:39:44, 751 root INFO Step: 29, Classifcation Loss: 33.68, Classification Acc: 20.00
14:39:44, 751 root INFO Step: 29, Matching Loss: 0.96, Matching Acc: 48.00
14:39:45, 392 root INFO Step: 30, Classifcation Loss: 34.68, Classification Acc: 15.00
14:39:45, 392 root INFO Step: 30, Matching Loss: 2.92, Matching Acc: 47.00
14:39:45, 992 root INFO Step: 31, Classifcation Loss: 34.23, Classification Acc: 22.00
14:39:45, 992 root INFO Step: 31, Matching Loss: 7.15, Matching Acc: 46.00
14:39:46, 602 root INFO Step: 32, Classifcation Loss: 32.93, Classification Acc: 25.00
14:39:46, 603 root INFO Step: 32, Matching Loss: 0.65, Matching Acc: 48.00
14:39:47, 175 root INFO Step: 33, Classifcation Loss: 32.95, Classification Acc: 24.00
14:39:47, 175 root INFO Step: 33, Matching Loss: 2.63, Matching Acc: 47.00
14:39:47, 767 root INFO Step: 34, Classifcation Loss: 34.29, Classification Acc: 17.00
14:39:47, 767 root INFO Step: 34, Matching Loss: 0.59, Matching Acc: 48.00
14:39:48, 372 root INFO Step: 35, Classifcation Loss: 32.93, Classification Acc: 28.00
14:39:48, 372 root INFO Step: 35, Matching Loss: 5.23, Matching Acc: 47.00
14:39:49, 1 root INFO Step: 36, Classifcation Loss: 33.54, Classification Acc: 21.00
14:39:49, 1 root INFO Step: 36, Matching Loss: 2.24, Matching Acc: 47.00
14:39:49, 571 root INFO Step: 37, Classifcation Loss: 33.71, Classification Acc: 18.00
14:39:49, 571 root INFO Step: 37, Matching Loss: 0.70, Matching Acc: 48.00
14:39:50, 136 root INFO Step: 38, Classifcation Loss: 32.87, Classification Acc: 26.00
14:39:50, 136 root INFO Step: 38, Matching Loss: 0.98, Matching Acc: 48.00
14:39:50, 695 root INFO Step: 39, Classifcation Loss: 34.05, Classification Acc: 19.00
14:39:50, 695 root INFO Step: 39, Matching Loss: 1.07, Matching Acc: 48.00
14:39:51, 267 root INFO Step: 40, Classifcation Loss: 33.99, Classification Acc: 21.00
14:39:51, 267 root INFO Step: 40, Matching Loss: 6.03, Matching Acc: 47.00
14:39:51, 904 root INFO Step: 41, Classifcation Loss: 34.05, Classification Acc: 23.00
14:39:51, 904 root INFO Step: 41, Matching Loss: 0.91, Matching Acc: 48.00
14:39:52, 499 root INFO Step: 42, Classifcation Loss: 32.91, Classification Acc: 24.00
14:39:52, 499 root INFO Step: 42, Matching Loss: 0.72, Matching Acc: 48.00
14:39:53, 67 root INFO Step: 43, Classifcation Loss: 33.46, Classification Acc: 25.00
14:39:53, 67 root INFO Step: 43, Matching Loss: 0.31, Matching Acc: 48.00
14:39:53, 610 root INFO Step: 44, Classifcation Loss: 32.04, Classification Acc: 30.00
14:39:53, 610 root INFO Step: 44, Matching Loss: 0.66, Matching Acc: 48.00
14:39:54, 186 root INFO Step: 45, Classifcation Loss: 32.49, Classification Acc: 28.00
14:39:54, 187 root INFO Step: 45, Matching Loss: 0.40, Matching Acc: 48.00
14:39:54, 779 root INFO Step: 46, Classifcation Loss: 33.61, Classification Acc: 23.00
14:39:54, 779 root INFO Step: 46, Matching Loss: 0.34, Matching Acc: 48.00
14:39:55, 375 root INFO Step: 47, Classifcation Loss: 33.40, Classification Acc: 19.00
14:39:55, 375 root INFO Step: 47, Matching Loss: 0.24, Matching Acc: 48.00
14:39:55, 916 root INFO Step: 48, Classifcation Loss: 34.46, Classification Acc: 18.00
14:39:55, 917 root INFO Step: 48, Matching Loss: 0.99, Matching Acc: 48.00
14:39:56, 499 root INFO Step: 49, Classifcation Loss: 33.91, Classification Acc: 21.00
14:39:56, 499 root INFO Step: 49, Matching Loss: 0.37, Matching Acc: 48.00
14:39:57, 87 root INFO Step: 50, Classifcation Loss: 33.55, Classification Acc: 22.00
14:39:57, 87 root INFO Step: 50, Matching Loss: 5.91, Matching Acc: 45.00
