task: 'textsum' # textsum, textsum-rl, lm
lang: 'zh' # 'en', 'zh'
seed: 1

vocab:
  tokenizer: 'char-zh' # ['word-en', 'word-zh', 'char-zh', 'spm' ]
  output_path: tokenizers/char-zh
  corpus: ['datasets/processed/LCSTS/train-doc.proc', 'datasets/processed/LCSTS/train-sum.proc']
  # corpus: ['datasets/processed/LCSTS/train-sent.proc']
  size: 10000
  min_freq: -1

  # if tokenizer == 'spm', override outer parameters
  spm:
    output_path: 'tokenizers/spm/LCSTS'
    model_type: bpe
    model_name: spm

data:
  dataset: LCSTS # LCSTS, LM
  raw_path: datasets/raw/
  processed_path: datasets/processed/
  tfrecord_path: datasets/tfrecord/
  type: ['train', 'valid', 'test']

  # task == 'LM'
  sent_len: 100

train:
  model_path: 'models/textsum-test/' # save and restore ckpt
  output_path: 'experiment' # export events and tf.Summary for Tensorboard
  max_ckpt: 5 # maximum checkpoints to be saved
  use_profile: True # profile model performance. run it with `tensorboard --logdir="output_path"`

  batch_size: 64
  learning_rate: 0.0002
  max_steps: 50000 # maximum training steps.
  steps_per_epoch: 50

  max_length: [200, 50] # if task=='textsum' max_length is [max_doc_length, max_sum_length], elif task=='lm' max_length is a int
  min_length: [10, 5] # if task=='textsum' min_length is [min_doc_length, min_sum_length], elif task=='lm' min_length is a int

  # max_length: 200 # if task=='textsum' max_length is [max_doc_length, max_sum_length], elif task=='lm' max_length is a int
  # min_length: 50 # if task=='textsum' min_length is [min_doc_length, min_sum_length], elif task=='lm' min_length is a int

model:
  # General options
  type: 'LSTM' # LSTM, GRU, Transformer
  n_layer: 2
  dim: 512
  is_bidirectional: True # whether use bi-RNN, if model's `type` is LSTM, GRU
  dropout_rate: 0.0

  # Transformer
  # if `type` is Transformer, fellowing attribute is required
  n_head: 8 # number of the heads, if model's `type` is Transformer
  ff_dim: 2048 # dimensions of linear layer if model's type is Transformer
  warmup_step: 4000 # steps for warm-up learning rate

  # Embedding layer
  emb_dim: 512
  emb_dropout: 0.0
  emb_trainable: True
  emb_init_weight: # whether initial weight from externel file
  share_inout: False # whether weights of word embedding are shared with output layer
  share_emb: False # whether weights are sharing between Encoder and Decoder
  use_scale: False # wether to scale embedding. To be applied when in `embedding` mode

  # RL
  rl_factor: 0.3 # used in RL tfraining





